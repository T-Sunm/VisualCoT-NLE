# VisualCoT for ViQVQA (Seeâ€“Thinkâ€“Confirm)

## ğŸš€ Quick Setup (One-time Installation)

This guide assumes you are working in the `~/workspace/VisualCoT-NLE` directory.

### 1. Activate Environment
```bash
source /home/research/my_envs/vllm_env/bin/activate
```
*Your prompt should change to `(vllm_env)`.*

### 2. Install vLLM
```bash
uv pip install vllm --torch-backend=auto
```

### 3. Install Dependencies

**VisualCoT-NLE Dependencies:**
```bash
uv add -r requirements.txt
```

**Describe-Anything (DAM) - External:**
```bash
cd external/describe-anything
pip install -v .
cd ../..
```

---

## ğŸ› ï¸ Usage Guide (Runtime)

âš ï¸ **Quan trá»ng**: Cáº§n má»Ÿ **3 terminal riÃªng biá»‡t** Ä‘á»ƒ cháº¡y cÃ¡c server vÃ  pipeline.

---

### Terminal 1: Start vLLM Server (LLM API)

```bash
# Activate mÃ´i trÆ°á»ng vLLM
source /home/research/my_envs/vllm_env/bin/activate

# Start vLLM vá»›i Vintern-1B (cho tiáº¿ng Viá»‡t)
vllm serve 5CD-AI/Vintern-1B-v3_5 \
    --port 1234 \
    --dtype auto \
    --gpu-memory-utilization 0.5 \
    --max-model-len 2048 \
    --trust-remote-code
```
> Server sáº½ cháº¡y táº¡i `http://localhost:1234`

---

### Terminal 2: Start DAM Server (Describe-Anything Model)

```bash
# Activate mÃ´i trÆ°á»ng vLLM (hoáº·c mÃ´i trÆ°á»ng riÃªng náº¿u cÃ³)
source /home/research/my_envs/vllm_env/bin/activate

# Di chuyá»ƒn vÃ o thÆ° má»¥c DAM
cd ~/workspace/VisualCoT-NLE/external/describe-anything

# Start DAM server
python dam_server.py \
    --model-path nvidia/DAM-3B \
    --conv-mode v1 \
    --prompt-mode focal_prompt \
    --port 8000
```
> Server sáº½ cháº¡y táº¡i `http://localhost:8000`

---

### Terminal 3: Run ViQVQA Pipeline

```bash
# Activate mÃ´i trÆ°á»ng chÃ­nh cá»§a project
source /home/research/my_envs/vllm_env/bin/activate

# Di chuyá»ƒn vÃ o thÆ° má»¥c project
cd ~/workspace/VisualCoT-NLE

# Cháº¡y pipeline
python src/pipeline.py \
    --config configs/experiments/vivqax_baseline.yaml \
    --limit 300 \
    --output results/vivqax_results.json
```

---

## ğŸ“Š Kiá»ƒm tra káº¿t quáº£

Sau khi cháº¡y xong, káº¿t quáº£ Ä‘Æ°á»£c lÆ°u táº¡i:
```
results/vivqax_results.json
```

---

## ğŸ“‚ Data Preparation

### Automated Download

#### 1. Download COCO 2014 Images (train + val)
```bash
bash scripts/download_data.sh
```
This will download and extract COCO 2014 images to `data/raw/coco/images/`.

#### 2. Download ViVQA-X Annotations (train + val + test)
```bash
python scripts/download_vivqax.py
```
This will download ViVQA-X annotations from Hugging Face to `data/raw/vivqa-x/annotations/`.

### Expected Directory Structure

After running both scripts, your data directory should look like:
```
data/
â””â”€â”€ raw/
    â”œâ”€â”€ coco/
    â”‚   â””â”€â”€ images/
    â”‚       â”œâ”€â”€ train2014/          # ~82,783 images
    â”‚       â””â”€â”€ val2014/            # ~40,504 images
    â”œâ”€â”€ vivqa-x/
    â”‚   â””â”€â”€ annotations/
    â”‚       â”œâ”€â”€ train.json          # Training annotations
    â”‚       â”œâ”€â”€ val.json            # Validation annotations
    â”‚       â””â”€â”€ test.json           # Test annotations
    â””â”€â”€ scene-graph/                # Pre-computed scene graphs (if needed)
```


---

## ğŸ“„ Citation
[Visual Chain-of-Thought Prompting for Knowledge-based Visual Reasoning](https://arxiv.org/abs/2301.05226)
```bibtex
@article{chen2023see,
  title={Visual Chain-of-Thought Prompting for Knowledge-based Visual Reasoning},
  author={Chen, Zhenfang and Zhou, Qinhong and Shen, Yikang and Hong, Yining and Sun, Zhiqing and Gutfreund, Dan and Gan, Chuang},
  journal={arXiv preprint arXiv:2301.05226},
  year={2023}
}
```